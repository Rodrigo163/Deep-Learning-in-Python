{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Basics of deep learning and neural networks\n",
    "Linear regression doesn't take into account interactions between the features. That's the big advantage of neural networks.\n",
    "\n",
    "Input layer vs output layer vs hidden layers.\n",
    "\n",
    "Gonna use example with trying to predict number of bank transactions for a client based on number of kids, age, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "The weights bet layers are the parameters we tune when training our model. They represent how strongly two nodes are related.\n",
    "\n",
    "Forward propagation = info goes from input to hidden layers to output layer. Dot product between input/node values and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](nn.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0': np.array([1,1]),\n",
    "          'node_1': np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])}\n",
    "\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "node_1_value = (input_data* weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "hidden_layer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions allow the model to capture non-linearities. This means we can capture different behaviour when going from 1->2 kids than when going from 4->5 kids.\n",
    "\n",
    "This functions are applied to the node inputs and produce the node output.\n",
    "\n",
    "For a long time the activation function was $tanh$ but now the industry standard is ReLU (Rectified Linead Activation).\n",
    "![](relu.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2382242525694254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we include activation functions in our previous code\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0': np.array([1,1]),\n",
    "          'node_1': np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_input = (input_data* weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "hidden_layer_values = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(input, 0)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the network to many observations/rows of data\n",
    "You'll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row*weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row*weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs*weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row,weights))\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "Many hidden layers are the feature that allows for such powerful stuff. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
