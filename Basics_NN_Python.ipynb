{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Basics of deep learning and neural networks\n",
    "Linear regression doesn't take into account interactions between the features. That's the big advantage of neural networks.\n",
    "\n",
    "Input layer vs output layer vs hidden layers.\n",
    "\n",
    "Gonna use example with trying to predict number of bank transactions for a client based on number of kids, age, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "The weights bet layers are the parameters we tune when training our model. They represent how strongly two nodes are related.\n",
    "\n",
    "Forward propagation = info goes from input to hidden layers to output layer. Dot product between input/node values and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](nn.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0': np.array([1,1]),\n",
    "          'node_1': np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])}\n",
    "\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "node_1_value = (input_data* weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "hidden_layer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions allow the model to capture non-linearities. This means we can capture different behaviour when going from 1->2 kids than when going from 4->5 kids.\n",
    "\n",
    "This functions are applied to the node inputs and produce the node output.\n",
    "\n",
    "For a long time the activation function was $tanh$ but now the industry standard is ReLU (Rectified Linead Activation).\n",
    "![](relu.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2382242525694254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we include activation functions in our previous code\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0': np.array([1,1]),\n",
    "          'node_1': np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_input = (input_data* weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "hidden_layer_values = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(input, 0)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the network to many observations/rows of data\n",
    "You'll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row*weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row*weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs*weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row,weights))\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "Many hidden layers are the feature that allows for such powerful stuff. \n",
    "\n",
    "Deep Networks internally build representations of patterns in the data and find increasingly complex patterns as we go deeper into the network.\n",
    "NN partially replace the need for feature engineering.\n",
    "\n",
    "Deep learning is sometimes calles representation learning because subsequent layers build incresingly sophisticated represetations of raw data until we can make predictions. \n",
    "\n",
    "Big advantage is that the modeler doesn't need to specify the interactions. Instead, when we train the model, the NN gets weights that find the relevant patterns to make better predictions.\n",
    "\n",
    "### Multi-layer neural networks\n",
    "In this exercise, you'll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. The input data has been preloaded as input_data. The nodes in the first hidden layer are called node_0_0 and node_0_1. Their weights are pre-loaded as weights['node_0_0'] and weights['node_0_1'] respectively.\n",
    "\n",
    "The nodes in the second hidden layer are called node_1_0 and node_1_1. Their weights are pre-loaded as weights['node_1_0'] and weights['node_1_1'] respectively.\n",
    "\n",
    "We then create a model output from the hidden nodes using weights pre-loaded as weights['output'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
