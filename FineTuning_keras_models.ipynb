{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning keras models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model optimization\n",
    "It is hard because we're optimizing 1000s of parameters with complex relationships. Updates may not improve our model meaningfully if the learning rate is too big or too small per example. \n",
    "The easiest way to see this is using the stochastic gradient descent algorithm since it uses fixed learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('titanic_all_numeric.csv.txt')\n",
    "predictors = df.drop(['survived'], axis=1).as_matrix()\n",
    "target = to_categorical(df.survived)\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/5\n",
      "891/891 [==============================] - 1s 980us/step - loss: 4.4653\n",
      "Epoch 2/5\n",
      "891/891 [==============================] - 0s 103us/step - loss: 4.4574\n",
      "Epoch 3/5\n",
      "891/891 [==============================] - 0s 108us/step - loss: 4.4494\n",
      "Epoch 4/5\n",
      "891/891 [==============================] - 0s 103us/step - loss: 4.4414\n",
      "Epoch 5/5\n",
      "891/891 [==============================] - 0s 112us/step - loss: 4.4333\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/5\n",
      "891/891 [==============================] - 1s 884us/step - loss: 1.7139\n",
      "Epoch 2/5\n",
      "891/891 [==============================] - 0s 103us/step - loss: 0.8800\n",
      "Epoch 3/5\n",
      "891/891 [==============================] - 0s 112us/step - loss: 0.7764\n",
      "Epoch 4/5\n",
      "891/891 [==============================] - 0s 108us/step - loss: 0.6162\n",
      "Epoch 5/5\n",
      "891/891 [==============================] - 0s 103us/step - loss: 0.6053\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/5\n",
      "891/891 [==============================] - 1s 1ms/step - loss: 6.0317\n",
      "Epoch 2/5\n",
      "891/891 [==============================] - 0s 117us/step - loss: 6.1867\n",
      "Epoch 3/5\n",
      "891/891 [==============================] - 0s 117us/step - loss: 6.1867\n",
      "Epoch 4/5\n",
      "891/891 [==============================] - 0s 121us/step - loss: 6.1867\n",
      "Epoch 5/5\n",
      "891/891 [==============================] - 0s 112us/step - loss: 6.1867\n"
     ]
    }
   ],
   "source": [
    "def get_new_model(n_cols):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    return(model)\n",
    "\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model(n_cols)\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer = my_optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target, epochs=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the learning rate is well tune we can run into the dying neuron problem:\n",
    "- A neuron takes a value less than 0 for all rows of the data.\n",
    "- relu produces output of 0 for any negative input. The slope is also 0. \n",
    "- this means the slopes of any weights flowing into that node are also zero.\n",
    "- Those weights don't get updated!\n",
    "Once a node starts always getting negative inputs it may continue only getting negative inputs.\n",
    "\n",
    "-> it contributes nothing to the model. --> DEAD NEURON\n",
    "\n",
    "A solution migh be using activation functions that don't vanish but this leads to the vanishing gradient problem. Per example tanh(). Occurs when many layers have very small slopes (due to being of the flat part of tanh).\n",
    "This worked fine in small networks but in deep ones updates to bp were close to 0.\n",
    "\n",
    "No perfect answer atm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "Model performance on training data is not a good measure on how it will perform on new data. \n",
    "\n",
    "We held out validation/test data to test performance.\n",
    "\n",
    "In many machine learning models we use k-cross validation but in deep networks were we are working with such large datasets it's not practicle. We'll generally use validation split. \n",
    "\n",
    "Single validation score is based on large amount of data and so it is reliable. \n",
    "\n",
    "To split data in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictor, target, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasonable way to train the networks is to continue if the validation score is improving and stop when it's not improving anymore. For this we can use early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create an early stopping monitor before fitting the model\n",
    "#patience = how many epoch we will keep updating even when there is no improvement. \n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "model.fit(predictor, target, validation_split=0.3, \n",
    "          epochs=20, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to measure performance of our network we can experiment with different architectures, +/- layers, +/- nodes.\n",
    "\n",
    "Creating a good model requires experimentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model accuracy on validation dataset\n",
    "Now it's your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You'll check the validation score in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors, target, validation_split = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with wider networks\n",
    "Now you know everything you need to begin experimenting with different models!\n",
    "\n",
    "A model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer.\n",
    "\n",
    "In this exercise you'll create a new model called model_2 which is similar to model_1, except it has 100 units in each hidden layer.\n",
    "\n",
    "After you create model_2, both models will be fitted, and a graph showing both models loss score at each epoch will be shown. We added the argument verbose=False in the fitting commands to print out fewer updates, since you will look at these graphically instead of as text.\n",
    "\n",
    "Because you are fitting two models, it will take a moment to see the outputs after you hit run, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation ='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation ='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation ='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics =['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to a network\n",
    "You've seen how to experiment with wider networks. In this exercise, you'll try a deeper network (more hidden layers).\n",
    "\n",
    "Once again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 50 units. You can see a summary of that model's structure printed out. You will create a similar network with 3 hidden layers (still keeping 50 units in each layer).\n",
    "\n",
    "This will again take a moment to fit both models, so you'll need to wait a few seconds to see the results after you run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(50, activation ='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation ='relu'))\n",
    "model_2.add(Dense(50, activation ='relu'))\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation ='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics =['accuracy'])\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about model capacity\n",
    "Model capacity is closely related to over and underfitting. Overfitting is the hability to fit patterns in the training data that are not relevant and will cause errors when applying the model to the test data. \n",
    "\n",
    "Underfitting is the opposite. The model fails to find patterns in the training data and then also fails in the test data. \n",
    "\n",
    "Model/Network capacity/complexity is related to how many nodes/layes we add to our network.\n",
    "\n",
    "![](cap.PNG)\n",
    "That's why validation scores are so important. We start with a small network and get validation score. Then we increase capacity until the validation score is not improving anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping up to images\n",
    "Recognizing handwritten digits with MNIST dataset.\n",
    "\n",
    "Each image is a 28x28 pixel grid.\n",
    "\n",
    "Deep learning with ipynb in the cloud:\n",
    "https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws\n",
    "\n",
    "### Building your own digit recognition model\n",
    "You've reached the final exercise of the course - you now know everything you need to build an accurate model to recognize handwritten digits!\n",
    "\n",
    "We've already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from keras are also pre-imported.\n",
    "\n",
    "To add an extra challenge, we've loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\n",
    "\n",
    "If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don't have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
