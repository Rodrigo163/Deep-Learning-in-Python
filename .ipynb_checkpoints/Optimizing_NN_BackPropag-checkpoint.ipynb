{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a neural network with backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we know what the output should be (like with labeled data) we'll try to minimize a cost or error function by changing the weights in the network. \n",
    "\n",
    "When we have multiple point things get harder. At any set of weights, there are many values of the error corresponding to the many points we make predictions for. \n",
    "\n",
    "Our Loss function will aggregate errors in predictions from many data points into a single number. It's a measure of the model's predictive performance. \n",
    "\n",
    "Example: Squared error loss function. We square all errors and get the mean of their sum. \n",
    "\n",
    "Goal is to find the weights that will minim. the loss function. We do this using gradient descent. \n",
    "\n",
    "### Scaling up to multiple data points\n",
    "You've seen how different weights will have different accuracies on a single prediction. But usually, you'll want to measure model accuracy on many points. You'll now write code to compare model accuracies for two different sets of weights, which have been stored as weights_0 and weights_1.\n",
    "\n",
    "input_data is a list of arrays. Each item in that list contains the data to make a single prediction. target_actuals is a list of numbers. Each item in that list is the actual value we are trying to predict.\n",
    "\n",
    "In this exercise, you'll use the mean_squared_error() function from sklearn.metrics. It takes the true values and the predicted values as arguments.\n",
    "\n",
    "You'll also use the preloaded predict_with_network() function, which takes an array of data as the first argument, and weights as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "If the slope is positive:\n",
    "- going opposite the slope means moving to lower numbers.\n",
    "- subtract the slope from the current value\n",
    "- too big a step might lead us too far\n",
    "The solution to this is the learning rate: update each weight by subtracting: $learning rate * slope$\n",
    "![](grad.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code to calculate slopes and update weights\n",
    "w = np.array([1, 2])\n",
    "inputd = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "\n",
    "pred= (w*inputd).sum()\n",
    "error = pred-target \n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gradient for our particular loss fn\n",
    "gradient = 2 * inputd * error\n",
    "\n",
    "w_updated = w - learning_rate * gradient\n",
    "pred_updated = (w_updated * inputd).sum()\n",
    "error_updated = pred_updated - target\n",
    "error_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structute to do multiple updates\n",
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = ____(____, ____, ____)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = ____ - ____ * ____\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = ____(____, ____, ____)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    ____\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "We already optimized our weights using gradient descent. Now we'll learn how to use backprop to calculate the slopes we need to optimize more complex deep Learning models. \n",
    "\n",
    "Backpropagation (BP) takes the prediction error from the output layer and propagates it backwards through the hidden layers all the way to the input layer. It calculates the slopes sequentially from the weights clossest to the output layer, through the hidden layers and finally back to the weights coming from the inputs. We then use those slopes to update the weights like we did before.\n",
    "\n",
    "It allows gradient descent to update all weights in nn and it comes from the chain rule.\n",
    "\n",
    "The big picture problem is trying to estimate the slope of the loss function wrt each weight. To do this we first need to have predictions and errors, so we always do forward propagation before bp.\n",
    "\n",
    "![](bp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "![](bp2.PNG)\n",
    "![](bp3.PNG)\n",
    "Remember which 3 things we need to multiply to find the gradient:\n",
    "- Node value feeding into that weight\n",
    "- Slope of activation fn for the node being fed into\n",
    "- Slope of loss fn wrt output node\n",
    "\n",
    "For the up right node going into up left we have\n",
    "- 0 \n",
    "- 6\n",
    "- 1 (from 6>0 -> derivative of relu = 1)\n",
    "![](bp4.PNG)\n",
    "\n",
    "RECAP:\n",
    "![](bp5.PNG)\n",
    "Then simply keep going with that cycle until we get to a flat part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "It is common to calculate slopes on only a subset of the data ('batch'). We use a different batch of data to calculate the next update.\n",
    "\n",
    "Once all data is used start over from the beginning. \n",
    "\n",
    "Each time through the training data is called an epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
